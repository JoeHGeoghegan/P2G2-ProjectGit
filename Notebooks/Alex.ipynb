{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import pmaw\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from libs.prep_methods import articles_pull, subreddit_pull, keyword_filter, articles_vader_analyzer, reddit_vader_analyzer, daily_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stockmarket_comments = subreddit_pull('stockmarket', limit = 10000, after = int(dt.datetime(2012, 1, 1, 0, 0).timestamp()), before = int(dt.datetime(2022, 6, 1, 0, 0).timestamp()))\n",
    "\n",
    "# securityanalysis_comments = subreddit_pull('securityanalysis', limit = 10000000, after = int(dt.datetime(2012, 1, 1, 0, 0).timestamp()), before = int(dt.datetime(2022, 6, 1, 0, 0).timestamp()))\n",
    "\n",
    "# algotrading_comments = subreddit_pull('algotrading', limit = 10000000, after = int(dt.datetime(2012, 1, 1, 0, 0).timestamp()), before = int(dt.datetime(2022, 6, 1, 0, 0).timestamp()))\n",
    "\n",
    "# wallstreetbets_comments = subreddit_pull('wallstreetbets', limit = 10000000, after = int(dt.datetime(2012, 1, 1, 0, 0).timestamp()), before = int(dt.datetime(2022, 6, 1, 0, 0).timestamp()))\n",
    "\n",
    "# stockmarket_comments.to_csv('./Data/Cleaned_Data/stockmarket_comments.csv')\n",
    "# securityanalysis_comments.to_csv('../../sandbox/securityanalysis_comments_large.csv')\n",
    "# algotrading_comments.to_csv('../../sandbox/algotrading_comments_large.csv')\n",
    "# wallstreetbets_comments.to_csv('./wallstreetbets_comments_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = pd.read_csv('./Data/Cleaned_Data/stock_data.csv', parse_dates = True, infer_datetime_format = True)\n",
    "stock_data['date'] = pd.to_datetime(stock_data['date'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "stock_data = stock_data.set_index('date')\n",
    "stock_data.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockmarket_comments = pd.read_csv('../../sandbox/stockmarket_comments_large.csv', lineterminator = '\\n', parse_dates = True, infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "'NFLX': ['NFLX', 'nflx', 'Netflix', 'netflix'],\n",
    "'FB': ['FB', 'fb', 'Facebook', 'facebook'],\n",
    "'UBER': ['UBER', 'uber', 'Uber'],\n",
    "'MCHP': ['MCHP', 'mchp', 'Microchip Technology'],\n",
    "'ABNB': ['ABNB', 'abnb', 'AirBnB', 'airbnb'],\n",
    "'FANG': ['FANG', 'fang', 'Diamondback Energy', 'diamondback energy', 'Diamondback', 'diamondback'],\n",
    "'MRO': ['MRO', 'mro', 'Marathon Oil', 'marathon oil'],\n",
    "'DVN': ['DVN', 'dvn', 'Devon Energy', 'devon energy'],\n",
    "'SPWR': ['SPWR', 'spwr', 'SunPower', 'Sunpower', 'sunpower'],\n",
    "'REGI': ['REGI', 'regi', 'Renewable Energy Group', 'renewable energy group'],\n",
    "'MTRX': ['MTRX', 'mtrx', 'McKinsey & Company', 'McKinsey & Co', 'Mckinsey & Co', 'McKinsey', 'Mckinsey', 'mckinsey'],\n",
    "'BLK': ['BLK', 'blk', 'BlackRock', 'Blackrock', 'blackrock'],\n",
    "'PYPL': ['PYPL', 'pypl', 'PayPal', 'Paypal', 'paypal'],\n",
    "'MELI': ['MELI', 'meli', 'MercadoLibre', 'Mercadolibre', 'mercadolibre'],\n",
    "'SOFI': ['SOFI', 'sofi', 'SoFi', 'Sofi']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asset_dataframe_list = []\n",
    "# for asset in keywords:\n",
    "\n",
    "#     asset_sec_sentiment = pd.read_csv(f'./Data/Cleaned_Data/SEC_sentiment_and_STOCKS/{asset}.csv', parse_dates = True, infer_datetime_format = True)\n",
    "#     asset_sec_sentiment = pd.read_csv(f'./Data/Cleaned_Data/SEC_sentiment_and_STOCKS/UBER.csv', parse_dates = True, infer_datetime_format = True)\n",
    "#     asset_sec_sentiment['date'] = pd.to_datetime(asset_sec_sentiment['date'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "#     asset_sec_sentiment = asset_sec_sentiment.set_index('date')\n",
    "#     asset_sec_sentiment.index.name = None\n",
    "\n",
    "#     asset_sec_sentiment = asset_sec_sentiment[['compound', 'pos', 'neu', 'neg']]\n",
    "#     asset_sec_sentiment = asset_sec_sentiment.rename(columns = {'pos': 'sec_positive_sentiment', 'neg': 'sec_negative_sentiment', 'neu': 'sec_neutral sentiment', 'compound': 'sec_compound_sentiment'})\n",
    "\n",
    "#     asset_stockmarket_comments = keyword_filter(stockmarket_comments.fillna(''), keywords[asset])\n",
    "#     asset_stockmarket_sentiment = daily_mean(reddit_vader_analyzer('stockmarket', asset_stockmarket_comments)).ffill()\n",
    "#     asset_stockmarket_sentiment['datetime'] = pd.to_datetime(asset_stockmarket_sentiment['datetime'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "#     asset_stockmarket_sentiment = asset_stockmarket_sentiment.set_index('datetime')\n",
    "#     asset_stockmarket_sentiment.index = asset_stockmarket_sentiment.index.date\n",
    "\n",
    "#     asset_prices_volume = stock_data[stock_data['ticker'] == asset].drop(columns = 'ticker')\n",
    "#     asset_prices_volume = asset_prices_volume[['volume', 'close']]\n",
    "\n",
    "#     asset_dataframe = pd.concat([asset_sec_sentiment, asset_stockmarket_sentiment, asset_prices_volume], axis = 1).ffill().dropna()\n",
    "#     asset_dataframe.to_csv(f'./Data/Cleaned_Data/{asset}.csv')\n",
    "\n",
    "#     asset_dataframe_list.append(asset_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_dataframe_list = []\n",
    "for asset in keywords:\n",
    "\n",
    "    asset_dataframe = pd.read_csv(f'./Data/Cleaned_Data/{asset}.csv', parse_dates = True, infer_datetime_format = True)\n",
    "    asset_dataframe['Unnamed: 0'] = pd.to_datetime(asset_dataframe['Unnamed: 0'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "    asset_dataframe = asset_dataframe.set_index('Unnamed: 0')\n",
    "    asset_dataframe.index.name = None\n",
    "    \n",
    "    asset_dataframe_list.append(asset_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix = pd.DataFrame(asset_dataframe_list[0])\n",
    "facebook = pd.DataFrame(asset_dataframe_list[1])\n",
    "uber = pd.DataFrame(asset_dataframe_list[2])\n",
    "microchip_technology = pd.DataFrame(asset_dataframe_list[3])\n",
    "airbnb = pd.DataFrame(asset_dataframe_list[4])\n",
    "diamondback_energy = pd.DataFrame(asset_dataframe_list[5])\n",
    "marathon_oil = pd.DataFrame(asset_dataframe_list[6])\n",
    "devon_energy = pd.DataFrame(asset_dataframe_list[7])\n",
    "sunpower_corp = pd.DataFrame(asset_dataframe_list[8])\n",
    "renewable_energy_group = pd.DataFrame(asset_dataframe_list[9])\n",
    "mckinsey_and_company = pd.DataFrame(asset_dataframe_list[10])\n",
    "blackrock = pd.DataFrame(asset_dataframe_list[11])\n",
    "paypal = pd.DataFrame(asset_dataframe_list[12])\n",
    "mercado_libre = pd.DataFrame(asset_dataframe_list[13])\n",
    "sofi = pd.DataFrame(asset_dataframe_list[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uber_financial_metrics = pd.read_csv(f'./Data/Cleaned_Data/SEC_Fin_Data_and_STOCKS/UBER.csv', parse_dates = True, infer_datetime_format = True)\n",
    "# uber_financial_metrics = uber_financial_metrics.rename(columns = {\n",
    "#     'date': 'date',\n",
    "#     'Revenues': 'revenue',\n",
    "#     'CostOfGoodsAndServiceExcludingDepreciationDepletionAndAmortization': 'cost_without_depletion_and_amortization', 'OperationsAndSupportExpense': 'operations_and_support_expense',\n",
    "#     'SellingAndMarketingExpense': 'selling_and_market_expense',\n",
    "#     'ResearchAndDevelopmentExpense': 'research_and_development_expense',\n",
    "#     'GeneralAndAdministrativeExpense': 'general_and_administrative_expense',\n",
    "#     'DepreciationDepletionAndAmortization': 'depreciation_depletion_and_amortization',\n",
    "#     'CostsAndExpenses': 'costs_and_expenses',\n",
    "#     'OperatingIncomeLoss': 'operating_income_loss',\n",
    "#     'InterestExpense': 'interest_expense',\n",
    "#     'NonoperatingIncomeExpense': 'nonoperating_income_expense', 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments': 'incomeloss_from_continuining_operations',\n",
    "#     'IncomeTaxExpenseBenefit': 'income_tax_expense_benefit',\n",
    "#     'IncomeLossFromEquityMethodInvestments': 'income_loss_from_equity_method_investments',\n",
    "#     'ProfitLoss': 'profit_loss',\n",
    "#     'Unnamed: 16': 'unnamed',\n",
    "#     'NetIncomeLoss': 'net_income_loss',\n",
    "#     'EarningsPerShareBasic': 'earnings_per_share_basic',\n",
    "#     'EarningsPerShareDiluted': 'earnings_per_share_diluted',\n",
    "#     'WeightedAverageNumberOfSharesOutstandingBasic': 'weighted_average_number_of_shares_outstanding_basic', \n",
    "#     'WeightedAverageNumberOfDilutedSharesOutstanding': 'weight_average_number_of_diluted_shares_outstanding', \n",
    "#     'NetIncomeLossAttributableToRedeemableNoncontrollingInterest': 'net_income_loss_attributable_to_redeemable_noncontrolling_interest'\n",
    "#     })\n",
    "\n",
    "# uber_financial_metrics = uber_financial_metrics.drop(columns = ['ticker', 'close', 'volume']) \n",
    "# uber_financial_metrics['date'] = pd.to_datetime(uber_financial_metrics['date'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "# uber_financial_metrics = uber_financial_metrics.set_index('date')\n",
    "# uber_financial_metrics.index = uber_financial_metrics.index.date\n",
    "# uber_financial_metrics.index.name = None\n",
    "\n",
    "# uber_with_sec = pd.concat([uber_financial_metrics, uber], axis = 1).ffill().dropna()\n",
    "# uber_with_sec.to_csv(f'./Data/Cleaned_Data/UBER_WITH_SEC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airbnb_financial_metrics = pd.read_csv(f'./Data/Cleaned_Data/SEC_Fin_Data_and_STOCKS/ABNB.csv', parse_dates = True, infer_datetime_format = True)\n",
    "# airbnb_financial_metrics = airbnb_financial_metrics.rename(columns = {\n",
    "#     'date': 'date',\n",
    "#     'Revenues': 'revenue',\n",
    "#     'CostOfGoodsAndServiceExcludingDepreciationDepletionAndAmortization': 'cost_without_depletion_and_amortization', 'OperationsAndSupportExpense': 'operations_and_support_expense',\n",
    "#     'SellingAndMarketingExpense': 'selling_and_market_expense',\n",
    "#     'ResearchAndDevelopmentExpense': 'research_and_development_expense',\n",
    "#     'GeneralAndAdministrativeExpense': 'general_and_administrative_expense',\n",
    "#     'DepreciationDepletionAndAmortization': 'depreciation_depletion_and_amortization',\n",
    "#     'CostsAndExpenses': 'costs_and_expenses',\n",
    "#     'OperatingIncomeLoss': 'operating_income_loss',\n",
    "#     'InterestExpense': 'interest_expense',\n",
    "#     'NonoperatingIncomeExpense': 'nonoperating_income_expense', 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments': 'incomeloss_from_continuining_operations',\n",
    "#     'IncomeTaxExpenseBenefit': 'income_tax_expense_benefit',\n",
    "#     'IncomeLossFromEquityMethodInvestments': 'income_loss_from_equity_method_investments',\n",
    "#     'ProfitLoss': 'profit_loss',\n",
    "#     'Unnamed: 16': 'unnamed',\n",
    "#     'NetIncomeLoss': 'net_income_loss',\n",
    "#     'EarningsPerShareBasic': 'earnings_per_share_basic',\n",
    "#     'EarningsPerShareDiluted': 'earnings_per_share_diluted',\n",
    "#     'WeightedAverageNumberOfSharesOutstandingBasic': 'weighted_average_number_of_shares_outstanding_basic', \n",
    "#     'WeightedAverageNumberOfDilutedSharesOutstanding': 'weight_average_number_of_diluted_shares_outstanding', \n",
    "#     'NetIncomeLossAttributableToRedeemableNoncontrollingInterest': 'net_income_loss_attributable_to_redeemable_noncontrolling_interest'\n",
    "#     })\n",
    "\n",
    "# airbnb_financial_metrics = airbnb_financial_metrics.drop(columns = ['ticker', 'close', 'volume']) \n",
    "# airbnb_financial_metrics['date'] = pd.to_datetime(airbnb_financial_metrics['date'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "# airbnb_financial_metrics = airbnb_financial_metrics.set_index('date')\n",
    "# airbnb_financial_metrics.index = airbnb_financial_metrics.index.date\n",
    "# airbnb_financial_metrics.index.name = None\n",
    "\n",
    "# airbnb_with_sec = pd.concat([airbnb_financial_metrics, airbnb], axis = 1).ffill().dropna()\n",
    "# airbnb_with_sec.to_csv(f'./Data/Cleaned_Data/ABNB_WITH_SEC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_with_sec = pd.read_csv(f'./Data/Cleaned_Data/UBER_WITH_SEC.csv', parse_dates = True, infer_datetime_format = True)\n",
    "uber_with_sec['Unnamed: 0'] = pd.to_datetime(uber_with_sec['Unnamed: 0'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "uber_with_sec = uber_with_sec.set_index('Unnamed: 0')\n",
    "uber_with_sec.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_with_sec = pd.read_csv(f'./Data/Cleaned_Data/ABNB_WITH_SEC.csv', parse_dates = True, infer_datetime_format = True)\n",
    "airbnb_with_sec['Unnamed: 0'] = pd.to_datetime(airbnb_with_sec['Unnamed: 0'], infer_datetime_format = True, errors = 'coerce', format = '%Y/%m/%d')\n",
    "airbnb_with_sec = airbnb_with_sec.set_index('Unnamed: 0')\n",
    "airbnb_with_sec.index.name = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pyvizenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aae7ae0ea0c990573bbc5423c43265214dd7797168b544b883b40c2ec9d805a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
