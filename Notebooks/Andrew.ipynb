{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985386aa-2faf-48f7-861f-4a19037ca0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import datetime as dt\n",
    "import alpaca_trade_api as tradeapi\n",
    "import json\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517496b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "tradeapi = REST(alpaca_key, alpaca_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49aa0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_trade_data(ticker, start, end, tradeapi): \n",
    "    ticker_df = tradeapi.get_bars(\n",
    "        ticker,\n",
    "        TimeFrame.Day,\n",
    "        start,\n",
    "        end\n",
    "    ).df\n",
    "    ticker_df['ticker'] = ticker\n",
    "\n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c35a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get_daily_trade_data('HD', '2022-01-03', '2022-01-07', tradeapi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2759568",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = pd.read_csv('./Data/Cleaned_Data/Ticker_library.csv')['Ticker'].to_list()\n",
    "\n",
    "def make_tickers_df(ticker_list, start_date_str, end_date_str, tradeapi):\n",
    "    ticker_dfs_list = [get_daily_trade_data(ticker, start_date_str, end_date_str, tradeapi) for ticker in ticker_list]\n",
    "    tickers_df = pd.concat(ticker_dfs_list, axis=0, join='outer')\n",
    "    tickers_df.index = tickers_df.index.date\n",
    "    tickers_df = tickers_df[['ticker','close','volume']]\n",
    "    return tickers_df\n",
    "    \n",
    "    #['symbol','close'].reset_index().rename(columns={'timestamp':'date'}).set_index('date').dt.date\n",
    "    #return tickers_df.index.dt.date.reset_index().rename(columns={\"timestamp\":\"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60530a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks_df = make_tickers_df(ticker_list, '2012-06-01', '2022-06-01', tradeapi)\n",
    "# stocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b29b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tickers_dict(ticker_list, start_date_str, end_date_str, tradeapi):\n",
    "    return {ticker: get_daily_trade_data(ticker, start_date_str, end_date_str, tradeapi) for ticker in ticker_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567b615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = ['AAPL', 'XOM']\n",
    "# my_tick_dict = make_tickers_dict(ticker_list, '2022-01-03', '2022-01-07', tradeapi)\n",
    "# my_tick_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e9e5c",
   "metadata": {},
   "source": [
    "## News Article Vader Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81f1ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\altma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\altma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from libs.drew_lib import *\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7ba719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"NEWSAPI_KEY\")\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "print(type(api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbf3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_df = pd.read_csv('./Data/Cleaned_Data/Ticker_library.csv')\n",
    "# company_df = company_df.rename(columns={'Company Name': 'Company_Name'})\n",
    "# company_dict = dict(zip(company_df.Company_Name, company_df.Ticker))\n",
    "# company_list = list(company_dict.keys())\n",
    "# tickers_list = company_dict.values()\n",
    "# ls = [type(item) for item in company_list]\n",
    "# print(ls)\n",
    "# tickers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a980e863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Picking out something new to watch from Netfli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-14</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>The platform could add the capability for come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>The streaming wars are no longer about which a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Streaming is a curious beast. One minute you'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>The timetables are accelerating\\r\\nIllustratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Aloy could be on her way to Netflix.\\r\\n\\n \\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-05-22</td>\n",
       "      <td>NFLX</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Over a three-hour dinner, Mr. Sarandos was cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Ncuti Gatwa, a star of the Netflix series Sex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-05-18</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Alongside 150 full time employees Netflix axed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>The news comes after Daredevils small cameo in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Finally well get to see exactly how Amazon app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Paramount+ has released the first trailer for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Netflix will reportedly roll out its c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Netflix debuts its newest offering, After Scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Apparently, Netflix still isnt ready to give u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>May 24 (Reuters) - Netflix Inc (NFLX.O) announ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-05-20</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Netflix is trying to build up hype for Strange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Doctor Strange in the Multiverse of Madness ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>Netflix is teeming with high quality fantasy. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date ticker                                               text\n",
       "0   2022-05-19   NFLX  Picking out something new to watch from Netfli...\n",
       "1   2022-05-14   NFLX  The platform could add the capability for come...\n",
       "2   2022-06-02   NFLX  The streaming wars are no longer about which a...\n",
       "3   2022-05-10   NFLX  Streaming is a curious beast. One minute you'l...\n",
       "4   2022-05-10   NFLX  The timetables are accelerating\\r\\nIllustratio...\n",
       "5   2022-05-26   NFLX  Aloy could be on her way to Netflix.\\r\\n\\n \\n\\...\n",
       "6   2022-05-22   NFLX                                                   \n",
       "7   2022-05-28   NFLX  Over a three-hour dinner, Mr. Sarandos was cha...\n",
       "8   2022-05-08   NFLX  Ncuti Gatwa, a star of the Netflix series Sex ...\n",
       "9   2022-05-18   NFLX  Alongside 150 full time employees Netflix axed...\n",
       "10  2022-05-19   NFLX  The news comes after Daredevils small cameo in...\n",
       "11  2022-05-13   NFLX  Finally well get to see exactly how Amazon app...\n",
       "12  2022-05-09   NFLX  Paramount+ has released the first trailer for ...\n",
       "13  2022-05-10   NFLX  <ul><li>Netflix will reportedly roll out its c...\n",
       "14  2022-06-02   NFLX  Netflix debuts its newest offering, After Scho...\n",
       "15  2022-06-06   NFLX  Apparently, Netflix still isnt ready to give u...\n",
       "16  2022-05-24   NFLX  May 24 (Reuters) - Netflix Inc (NFLX.O) announ...\n",
       "17  2022-05-20   NFLX  Netflix is trying to build up hype for Strange...\n",
       "18  2022-06-02   NFLX  Doctor Strange in the Multiverse of Madness ha...\n",
       "19  2022-05-16   NFLX  Netflix is teeming with high quality fantasy. ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nflx_news_df = form_df('Netflix')\n",
    "nflx_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4238f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\altma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d526b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vader(df):\n",
    "#     company_list = company_dict.keys()\n",
    "\n",
    "#     df_list = []\n",
    "#     for company in company_list:\n",
    "#         try:\n",
    "#             df_list.append(form_df(company))\n",
    "#         except:\n",
    "#             continue\n",
    "#     news_df = pd.concat(df_list, axis=0, join='outer')\n",
    "    \n",
    "#     return news_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7bbe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_analyzer(df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['text']]\n",
    "    df['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df['text']]\n",
    "    df['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df['text']]\n",
    "    df['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df['text']]\n",
    "\n",
    "    df['date'] = pd.to_datetime(\n",
    "    df['date'],\n",
    "    infer_datetime_format = True,\n",
    "    utc = True    \n",
    "    )\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d55223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def daily_sentiment(df):\n",
    "#     df = vader_analyzer(df)\n",
    "\n",
    "#     df['date'] = pd.to_datetime(\n",
    "#     df['date'],\n",
    "#     infer_datetime_format = True,\n",
    "#     utc = True    \n",
    "#     )\n",
    "    \n",
    "#     return df.groupby('date')['compound','pos','neu','neg'].mean()\n",
    "\n",
    "\n",
    "# # date to datetime\n",
    "\n",
    "#     df['date'] = pd.to_datetime(\n",
    "#     df['date'],\n",
    "#     infer_datetime_format = True,\n",
    "#     utc = True    \n",
    "#     )\n",
    "#     df['date'] = df['date'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622e8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = vader_analyzer(netflix_news_df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eb7d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altma\\AppData\\Local\\Temp\\ipykernel_10048\\3887123157.py:4: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  news_sent_avg = vader_news_df.groupby(['ticker','date'])['ticker','pos','neg','neu','compound'].mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "news_df_list = [form_df(company) for company in company_list]\n",
    "vader_news_df_list = [vader_analyzer(df) for df in news_df_list]\n",
    "vader_news_df = pd.concat(vader_news_df_list, axis=0, join='inner')\n",
    "news_sent_avg = vader_news_df.groupby(['ticker','date'])['ticker','pos','neg','neu','compound'].mean().reset_index()\n",
    "news_sent_avg = news_sent_avg[['date','ticker','pos','neg','neu','compound']]\n",
    "\n",
    "\n",
    "news_sent_avg.to_csv('../Notebooks/Data/Cleaned_Data/news_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d71c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform sentiment analysis and find average sentiment by date\n",
    "\n",
    "def daily_sentiment(df):\n",
    "    vader_df = vader_analyzer(df)\n",
    "    vader_df = vader_df.groupby(['ticker','date'])['ticker','pos','neg','neu','compound'].mean().reset_index()\n",
    "    vader_df = vader_df[['date','ticker','pos','neg','neu','compound']]\n",
    "    return vader_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16d4ca",
   "metadata": {},
   "source": [
    "## Helper Functions for PuttingItTogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4d73d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter through all csv files in Cleaned_Data and return dataframe for a ticker\n",
    "\n",
    "def stock_picker(ticker):\n",
    "\n",
    "    stock_df_list = []\n",
    "    file_path = '../Notebooks/Data/Cleaned_Data'\n",
    "\n",
    "    for filename in os.listdir(file_path):\n",
    "        if filename.endswith(\".csv\") and filename != 'Ticker_library.csv' or 'news_sentiment.csv':\n",
    "            csv_df = pd.read_csv(file_path +'/'+ filename, parse_dates=True, infer_datetime_format=True,index_col='datetime')\n",
    "            #csv_df = csv_df.loc[csv_df['ticker'] == ticker]\n",
    "            #csv_df.drop(columns='ticker',axis=1,inplace=True)\n",
    "            #csv_df.set_index('date',inplace=True)\n",
    "            stock_df_list.append(csv_df)\n",
    "    all_ticker_data_df = pd.concat(stock_df_list,axis=1,join='outer')\n",
    "    #all_ticker_data_df.insert(0, 'ticker', ticker)\n",
    "    return all_ticker_data_df.drop(columns='Unnamed: 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8253e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_bin(df):\n",
    "\n",
    "    bins = [-1000, -0.08, -0.03, -0.0000001, 0.0000001, 0.03, 0.08, 1000]\n",
    "    labels = ['large loss','medium loss','small loss','no gain/loss','small gain','medium gain','large gain']\n",
    "    \n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['return_bin'] = pd.cut(df['returns'], bins=bins, labels=labels)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff48b0",
   "metadata": {},
   "source": [
    "## Data prep for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35efcb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fa13aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles_compound_sentiment</th>\n",
       "      <th>articles_positive_sentiment</th>\n",
       "      <th>articles_neutral_sentiment</th>\n",
       "      <th>articles_negative_sentiment</th>\n",
       "      <th>stockmarket_compound_sentiment</th>\n",
       "      <th>stockmarket_positive_sentiment</th>\n",
       "      <th>stockmarket_neutral_sentiment</th>\n",
       "      <th>stockmarket_negative_sentiment</th>\n",
       "      <th>securityanalysis_compound_sentiment</th>\n",
       "      <th>securityanalysis_positive_sentiment</th>\n",
       "      <th>securityanalysis_neutral_sentiment</th>\n",
       "      <th>securityanalysis_negative_sentiment</th>\n",
       "      <th>algotrading_compound_sentiment</th>\n",
       "      <th>algotrading_positive_sentiment</th>\n",
       "      <th>algotrading_neutral_sentiment</th>\n",
       "      <th>algotrading_negative_sentiment</th>\n",
       "      <th>wallstreetbets_compound_sentiment</th>\n",
       "      <th>wallstreetbets_positive_sentiment</th>\n",
       "      <th>wallstreetbets_neutral_sentiment</th>\n",
       "      <th>wallstreetbets_negative_sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.319520</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.252993</td>\n",
       "      <td>0.070267</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212671</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.815857</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.573991</td>\n",
       "      <td>0.184636</td>\n",
       "      <td>0.744364</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033250</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.074563</td>\n",
       "      <td>0.847313</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.284350</td>\n",
       "      <td>0.114167</td>\n",
       "      <td>0.825133</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>0.230365</td>\n",
       "      <td>0.08665</td>\n",
       "      <td>0.8853</td>\n",
       "      <td>0.02805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            articles_compound_sentiment  articles_positive_sentiment  \\\n",
       "datetime                                                               \n",
       "2013-01-01                     0.000000                      0.00000   \n",
       "2013-01-02                     0.000000                      0.00000   \n",
       "2013-01-03                     0.000000                      0.00000   \n",
       "2013-01-04                     0.000000                      0.00000   \n",
       "2013-01-05                     0.000000                      0.00000   \n",
       "...                                 ...                          ...   \n",
       "2015-02-11                     0.000000                      0.00000   \n",
       "2015-02-12                     0.000000                      0.00000   \n",
       "2015-02-13                     0.000000                      0.00000   \n",
       "2015-02-14                     0.000000                      0.00000   \n",
       "2022-06-06                     0.230365                      0.08665   \n",
       "\n",
       "            articles_neutral_sentiment  articles_negative_sentiment  \\\n",
       "datetime                                                              \n",
       "2013-01-01                      0.0000                      0.00000   \n",
       "2013-01-02                      0.0000                      0.00000   \n",
       "2013-01-03                      0.0000                      0.00000   \n",
       "2013-01-04                      0.0000                      0.00000   \n",
       "2013-01-05                      0.0000                      0.00000   \n",
       "...                                ...                          ...   \n",
       "2015-02-11                      0.0000                      0.00000   \n",
       "2015-02-12                      0.0000                      0.00000   \n",
       "2015-02-13                      0.0000                      0.00000   \n",
       "2015-02-14                      0.0000                      0.00000   \n",
       "2022-06-06                      0.8853                      0.02805   \n",
       "\n",
       "            stockmarket_compound_sentiment  stockmarket_positive_sentiment  \\\n",
       "datetime                                                                     \n",
       "2013-01-01                       -0.319520                        0.045600   \n",
       "2013-01-02                        0.252993                        0.070267   \n",
       "2013-01-03                        0.212671                        0.106000   \n",
       "2013-01-04                        0.573991                        0.184636   \n",
       "2013-01-05                        0.033250                        0.066500   \n",
       "...                                    ...                             ...   \n",
       "2015-02-11                        0.000000                        0.000000   \n",
       "2015-02-12                        0.000000                        0.000000   \n",
       "2015-02-13                        0.105544                        0.074563   \n",
       "2015-02-14                        0.284350                        0.114167   \n",
       "2022-06-06                        0.000000                        0.000000   \n",
       "\n",
       "            stockmarket_neutral_sentiment  stockmarket_negative_sentiment  \\\n",
       "datetime                                                                    \n",
       "2013-01-01                       0.838600                        0.115800   \n",
       "2013-01-02                       0.919000                        0.010667   \n",
       "2013-01-03                       0.815857                        0.078000   \n",
       "2013-01-04                       0.744364                        0.071000   \n",
       "2013-01-05                       0.860500                        0.073000   \n",
       "...                                   ...                             ...   \n",
       "2015-02-11                       0.000000                        0.000000   \n",
       "2015-02-12                       0.000000                        0.000000   \n",
       "2015-02-13                       0.847313                        0.078125   \n",
       "2015-02-14                       0.825133                        0.060800   \n",
       "2022-06-06                       0.000000                        0.000000   \n",
       "\n",
       "            securityanalysis_compound_sentiment  \\\n",
       "datetime                                          \n",
       "2013-01-01                                  0.0   \n",
       "2013-01-02                                  0.0   \n",
       "2013-01-03                                  0.0   \n",
       "2013-01-04                                  0.0   \n",
       "2013-01-05                                  0.0   \n",
       "...                                         ...   \n",
       "2015-02-11                                  0.0   \n",
       "2015-02-12                                  0.0   \n",
       "2015-02-13                                  0.0   \n",
       "2015-02-14                                  0.0   \n",
       "2022-06-06                                  0.0   \n",
       "\n",
       "            securityanalysis_positive_sentiment  \\\n",
       "datetime                                          \n",
       "2013-01-01                                  0.0   \n",
       "2013-01-02                                  0.0   \n",
       "2013-01-03                                  0.0   \n",
       "2013-01-04                                  0.0   \n",
       "2013-01-05                                  0.0   \n",
       "...                                         ...   \n",
       "2015-02-11                                  0.0   \n",
       "2015-02-12                                  0.0   \n",
       "2015-02-13                                  0.0   \n",
       "2015-02-14                                  0.0   \n",
       "2022-06-06                                  0.0   \n",
       "\n",
       "            securityanalysis_neutral_sentiment  \\\n",
       "datetime                                         \n",
       "2013-01-01                                 0.0   \n",
       "2013-01-02                                 0.0   \n",
       "2013-01-03                                 0.0   \n",
       "2013-01-04                                 0.0   \n",
       "2013-01-05                                 0.0   \n",
       "...                                        ...   \n",
       "2015-02-11                                 0.0   \n",
       "2015-02-12                                 0.0   \n",
       "2015-02-13                                 0.0   \n",
       "2015-02-14                                 0.0   \n",
       "2022-06-06                                 0.0   \n",
       "\n",
       "            securityanalysis_negative_sentiment  \\\n",
       "datetime                                          \n",
       "2013-01-01                                  0.0   \n",
       "2013-01-02                                  0.0   \n",
       "2013-01-03                                  0.0   \n",
       "2013-01-04                                  0.0   \n",
       "2013-01-05                                  0.0   \n",
       "...                                         ...   \n",
       "2015-02-11                                  0.0   \n",
       "2015-02-12                                  0.0   \n",
       "2015-02-13                                  0.0   \n",
       "2015-02-14                                  0.0   \n",
       "2022-06-06                                  0.0   \n",
       "\n",
       "            algotrading_compound_sentiment  algotrading_positive_sentiment  \\\n",
       "datetime                                                                     \n",
       "2013-01-01                             0.0                             0.0   \n",
       "2013-01-02                             0.0                             0.0   \n",
       "2013-01-03                             0.0                             0.0   \n",
       "2013-01-04                             0.0                             0.0   \n",
       "2013-01-05                             0.0                             0.0   \n",
       "...                                    ...                             ...   \n",
       "2015-02-11                             0.0                             0.0   \n",
       "2015-02-12                             0.0                             0.0   \n",
       "2015-02-13                             0.0                             0.0   \n",
       "2015-02-14                             0.0                             0.0   \n",
       "2022-06-06                             0.0                             0.0   \n",
       "\n",
       "            algotrading_neutral_sentiment  algotrading_negative_sentiment  \\\n",
       "datetime                                                                    \n",
       "2013-01-01                            0.0                             0.0   \n",
       "2013-01-02                            0.0                             0.0   \n",
       "2013-01-03                            0.0                             0.0   \n",
       "2013-01-04                            0.0                             0.0   \n",
       "2013-01-05                            0.0                             0.0   \n",
       "...                                   ...                             ...   \n",
       "2015-02-11                            0.0                             0.0   \n",
       "2015-02-12                            0.0                             0.0   \n",
       "2015-02-13                            0.0                             0.0   \n",
       "2015-02-14                            0.0                             0.0   \n",
       "2022-06-06                            0.0                             0.0   \n",
       "\n",
       "            wallstreetbets_compound_sentiment  \\\n",
       "datetime                                        \n",
       "2013-01-01                                0.0   \n",
       "2013-01-02                                0.0   \n",
       "2013-01-03                                0.0   \n",
       "2013-01-04                                0.0   \n",
       "2013-01-05                                0.0   \n",
       "...                                       ...   \n",
       "2015-02-11                                0.0   \n",
       "2015-02-12                                0.0   \n",
       "2015-02-13                                0.0   \n",
       "2015-02-14                                0.0   \n",
       "2022-06-06                                0.0   \n",
       "\n",
       "            wallstreetbets_positive_sentiment  \\\n",
       "datetime                                        \n",
       "2013-01-01                                0.0   \n",
       "2013-01-02                                0.0   \n",
       "2013-01-03                                0.0   \n",
       "2013-01-04                                0.0   \n",
       "2013-01-05                                0.0   \n",
       "...                                       ...   \n",
       "2015-02-11                                0.0   \n",
       "2015-02-12                                0.0   \n",
       "2015-02-13                                0.0   \n",
       "2015-02-14                                0.0   \n",
       "2022-06-06                                0.0   \n",
       "\n",
       "            wallstreetbets_neutral_sentiment  \\\n",
       "datetime                                       \n",
       "2013-01-01                               0.0   \n",
       "2013-01-02                               0.0   \n",
       "2013-01-03                               0.0   \n",
       "2013-01-04                               0.0   \n",
       "2013-01-05                               0.0   \n",
       "...                                      ...   \n",
       "2015-02-11                               0.0   \n",
       "2015-02-12                               0.0   \n",
       "2015-02-13                               0.0   \n",
       "2015-02-14                               0.0   \n",
       "2022-06-06                               0.0   \n",
       "\n",
       "            wallstreetbets_negative_sentiment  \n",
       "datetime                                       \n",
       "2013-01-01                                0.0  \n",
       "2013-01-02                                0.0  \n",
       "2013-01-03                                0.0  \n",
       "2013-01-04                                0.0  \n",
       "2013-01-05                                0.0  \n",
       "...                                       ...  \n",
       "2015-02-11                                0.0  \n",
       "2015-02-12                                0.0  \n",
       "2015-02-13                                0.0  \n",
       "2015-02-14                                0.0  \n",
       "2022-06-06                                0.0  \n",
       "\n",
       "[776 rows x 20 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_sent_df = pd.read_csv('../Notebooks/Data/Cleaned_Data/apple_sentiment.csv', infer_datetime_format=True, parse_dates=True,index_col='datetime')\n",
    "aapl_sent_df = aapl_sent_df.fillna(0)\n",
    "aapl_sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7543c4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [close, volume]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df = pd.read_csv('../Notebooks/Data/Cleaned_Data/stock_data.csv',infer_datetime_format=True,parse_dates=True)\n",
    "stock_df = stock_df.loc[stock_df['ticker']=='AAPL']\n",
    "stock_df = stock_df.set_index('date').drop(columns=['ticker'])\n",
    "stock_df\n",
    "#model_df = pd.concat([aapl_sent_df,stock_df],axis=1,join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681af540",
   "metadata": {},
   "outputs": [],
   "source": [
    "nflx_df = stock_picker('NFLX')\n",
    "#nflx_df = return_bin(nflx_df)\n",
    "#nflx_df = nflx_df[nflx_df['return_bin'].notna()].fillna(0)\n",
    "\n",
    "nflx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26acce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nflx_df.copy()\n",
    "X = nflx_df.drop(columns=['return_bin','ticker'])\n",
    "y = nflx_df['return_bin'].values\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b834b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train)\n",
    "\n",
    "y_train_enc = enc.transform(y_train).toarray()\n",
    "y_test_enc = enc.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12325bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "number_units = 30\n",
    "dropout_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 3s 7ms/step - loss: nan\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train_scaled.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train_enc, epochs=20, shuffle=False, batch_size=90, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test_scaled, y_test_enc, verbose=0)\n",
    "\n",
    "# Make predictions using the testing data X_test\n",
    "predicted = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905687ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\altma\\OneDrive\\Documents\\FinTechBootCamp\\projects\\P2G2-ProjectGit\\Notebooks\\Andrew.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/altma/OneDrive/Documents/FinTechBootCamp/projects/P2G2-ProjectGit/Notebooks/Andrew.ipynb#ch0000063?line=0'>1</a>\u001b[0m predicted \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39;49minverse_transform(predicted)\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/altma/OneDrive/Documents/FinTechBootCamp/projects/P2G2-ProjectGit/Notebooks/Andrew.ipynb#ch0000063?line=1'>2</a>\u001b[0m results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/altma/OneDrive/Documents/FinTechBootCamp/projects/P2G2-ProjectGit/Notebooks/Andrew.ipynb#ch0000063?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mActual\u001b[39m\u001b[39m\"\u001b[39m: y_test\u001b[39m.\u001b[39mactivity\u001b[39m.\u001b[39mvalues,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/altma/OneDrive/Documents/FinTechBootCamp/projects/P2G2-ProjectGit/Notebooks/Andrew.ipynb#ch0000063?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPredicted\u001b[39m\u001b[39m\"\u001b[39m: predicted\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/altma/OneDrive/Documents/FinTechBootCamp/projects/P2G2-ProjectGit/Notebooks/Andrew.ipynb#ch0000063?line=4'>5</a>\u001b[0m })\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/altma/OneDrive/Documents/FinTechBootCamp/projects/P2G2-ProjectGit/Notebooks/Andrew.ipynb#ch0000063?line=5'>6</a>\u001b[0m results\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\altma\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:582\u001b[0m, in \u001b[0;36mOneHotEncoder.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=561'>562</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=562'>563</a>\u001b[0m \u001b[39mConvert the data back to the original representation.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=563'>564</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=578'>579</a>\u001b[0m \u001b[39m    Inverse transformed array.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=579'>580</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=580'>581</a>\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=581'>582</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=583'>584</a>\u001b[0m n_samples, _ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/preprocessing/_encoders.py?line=584'>585</a>\u001b[0m n_features \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_)\n",
      "File \u001b[1;32mc:\\Users\\altma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=793'>794</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=794'>795</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=795'>796</a>\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=796'>797</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=798'>799</a>\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=799'>800</a>\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=801'>802</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=802'>803</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\altma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=106'>107</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=107'>108</a>\u001b[0m         allow_nan\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=108'>109</a>\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=109'>110</a>\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=110'>111</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=111'>112</a>\u001b[0m     ):\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=112'>113</a>\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=113'>114</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=114'>115</a>\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=115'>116</a>\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=116'>117</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=117'>118</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=118'>119</a>\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/altma/anaconda3/lib/site-packages/sklearn/utils/validation.py?line=119'>120</a>\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "predicted = enc.inverse_transform(predicted).flatten().tolist()\n",
    "results = pd.DataFrame({\n",
    "    \"Actual\": y_test.activity.values,\n",
    "    \"Predicted\": predicted\n",
    "})\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e569e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f688966fe16c5b1d1f8096f0874425497eb397fd30a583fa64973664e1f67766"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
